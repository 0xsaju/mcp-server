# Optional dependencies for better performance
# Install these after core dependencies

# Flash Attention for faster transformer inference
# Note: Requires CUDA and may need specific hardware compatibility
flash-attn>=2.0.0

# Development and utilities
pytest>=7.0.0
black>=23.0.0
isort>=5.12.0
